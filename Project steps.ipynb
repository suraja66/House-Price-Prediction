{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78963d9c-28e1-4a85-aa07-ff8e5f765a9e",
   "metadata": {},
   "source": [
    "Project :  House Price Prediction \n",
    "\n",
    "1. install/import all required ML libraries/modules.\n",
    "   \n",
    "2. import dataset.\n",
    "   \n",
    "3. Data Preprocessing\n",
    "   - Categorize the features depending on their datatype (int, float, object) and then calculate the number of them\n",
    "    \n",
    "4. Exploratory Data Analysis :\n",
    "   - EDA refers to the deep analysis of data so as to discover different patterns and spot anomalies/outliers.\n",
    "     Before making inferences from data it is essential to examine all your variables.\n",
    "   - So here let’s make a heatmap using seaborn library.\n",
    "   - if you get errors to generate hetmap, error like 'could not convert dtype' then you have to\n",
    "     remove columns which are not required for heatmap(Generally heatmap automatically removes those columns, but if you get above error you need to\n",
    "     identify and remove those columns by using drop()). \n",
    "\n",
    "5. Identifying categorical columns : (From Origional DataFrame)\n",
    "   - The variable object_cols is expected to contain a list of column names that are categorical. \n",
    "     (typically columns with string or object data types).\n",
    "     \n",
    "6. Collect unique value counts for each categorical feature : \n",
    "   - A list called unique_values is created to store the count of unique values for each categorical column.\n",
    "   - A for loop iterates over each column name in object_cols.\n",
    "   - For each column, dataset[col].unique().size calculates the number of unique values, which is then appended to the unique_values list.\n",
    "   \n",
    "7. Plotting the bar plot :\n",
    "   - To findout the actual count of each category we can plot the bargraph of each four features separately\n",
    "\n",
    "8. Data Cleaning :\n",
    "   - Data Cleaning is the way to improvise the data or remove incorrect, corrupted or irrelevant data.\n",
    "   - As in our dataset, there are some columns that are not important and irrelevant for the model training. So, we can drop that column before\n",
    "     training. There are 2 approaches to dealing with empty/null values\n",
    "     \n",
    "     1. We can easily delete the column/row (if the feature or record is not much important).\n",
    "        \n",
    "     2. Filling the empty slots with mean/mode/0/NA/etc. (depending on the dataset requirement).\n",
    "        (Target column empty values replaced with their mean values to get data destribution symmetric. )\n",
    "\n",
    "9. OneHotEncoder – For Label categorical features : (to be applied on list of object categorial list)\n",
    "   - One hot Encoding is the best way to convert categorical data into binary vectors. \n",
    "   - This maps the values to integer values. By using OneHotEncoder, we can easily convert object data into int. \n",
    "   - So for that, firstly we have to collect all the features which have the object datatype.\n",
    "\n",
    "10. Splitting Dataset into Training and Testing :\n",
    "   - Split the training set into training and validation set\n",
    "\n",
    "11. Model and Accuracy\n",
    "   - As we have to train the model to determine the continuous values, so we will be using these regression models.\n",
    "     1. SVM-Support Vector Machine : SVM can be used for both regression and classification model. It finds the hyperplane in the n-dimensional plane.\n",
    "     2. Random Forest Regressor : Random Forest is an ensemble technique that uses multiple of decision trees and can be used for both regression and\n",
    "        classification tasks.\n",
    "     4. Linear Regressor : Linear Regression predicts the final output-dependent value based on the given independent features. Like, here we have to\n",
    "        predict SalePrice depending on features like MSSubClass, YearBuilt, BldgType, Exterior1st etc.\n",
    "\n",
    "   - And To calculate loss we will be using the mean_absolute_percentage_error module. It can easily be imported by using sklearn library. \n",
    "\n",
    "12. Conclusion.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
